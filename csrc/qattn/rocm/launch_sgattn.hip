// !!! This is a file automatically generated by hipify!!!
#include "dispatch_utils.h"
#include "sgattn.hip"
#include <hip/hip_fp8.h> 
#include <c10/core/DeviceGuard.h> 

#include <hip/hip_runtime.h>
#if defined(USE_ROCM)
  #include <ATen/hip/HIPContext.h>
  #define TORCH_STREAM  at::hip::getCurrentHIPStream().stream()
  #define DEVICE_GUARD  at::hip::HIPGuard device_guard(query.device());
#else
  #include <ATen/hip/HIPContext.h>
  #define TORCH_STREAM  at::hip::getCurrentHIPStreamMasqueradingAsCUDA().stream()
  #define DEVICE_GUARD  at::hip::HIPGuardMasqueradingAsCUDA device_guard(query.device());
#endif

#include <atomic>
#include <fcntl.h>
#include <sys/stat.h>
#include <unistd.h>
#include <fstream>
#include <iostream>
#include <stdio.h>
#include <torch/serialize.h>
#include <torch/serialize/tensor.h>
#include <ATen/ATen.h>
#include <ATen/Functions.h>
#include "args.hpp"


#ifdef __ROCM_ARCH_GFX942
  using fp8_type = __hip_fp8_e4m3_fnuz;
  namespace Params = gfx9Params;
#else
  using fp8_type = __hip_fp8_e4m3;
  namespace Params = gfx11Params;
#endif

torch::Tensor launch_sgattn(torch::Tensor query,
                    torch::Tensor key,
                    torch::Tensor value,
                    torch::Tensor output,
                    torch::Tensor query_scale,
                    torch::Tensor key_scale,
                    torch::Tensor value_scale,
                    // torch::Tensor value_mean,
                    int tensor_layout,
                    int is_causal,
                    int qk_quant_gran,
                    float sm_scale,
                    int return_lse)
{
  CHECK_CUDA(query);
  CHECK_CUDA(key);
  CHECK_CUDA(value);
  CHECK_CUDA(output);
  CHECK_CUDA(query_scale);
  CHECK_CUDA(key_scale);
  CHECK_CUDA(value_scale);

  CHECK_LASTDIM_CONTIGUOUS(query);
  CHECK_LASTDIM_CONTIGUOUS(key);
  CHECK_CONTIGUOUS(value); // ensure value is contiguous to prevent troubles in the kernel
  CHECK_LASTDIM_CONTIGUOUS(output);
  CHECK_CONTIGUOUS(query_scale);
  CHECK_CONTIGUOUS(key_scale);
  CHECK_CONTIGUOUS(value_scale);

  CHECK_DTYPE(query, torch::kInt8);
  CHECK_DTYPE(key, torch::kInt8);
  // TODO: how to check fp8 data type?
  CHECK_DTYPE(query_scale, torch::kFloat32);
  CHECK_DTYPE(key_scale, torch::kFloat32);
  CHECK_DTYPE(value_scale, torch::kFloat32);

  CHECK_DIMS(query, 4);
  CHECK_DIMS(key, 4);
  CHECK_DIMS(value, 4);
  CHECK_DIMS(output, 4);
  CHECK_DIMS(query_scale, 3);
  CHECK_DIMS(key_scale, 3);
  CHECK_DIMS(value_scale, 3);

  auto pad_seq128 = [](const torch::Tensor& x) {
    TORCH_CHECK(x.dim() == 4, "expect [B,H,Seq,D]");
    const int64_t seq = x.size(2);
    const int64_t pad = (128 - (seq % 128)) % 128;
    if (pad == 0) return x.contiguous();
    return at::constant_pad_nd(x, {0,0, 0,pad}, /*value=*/0).contiguous();
  };

    auto Q_pad = pad_seq128(query);   // [B,Hq,M_pad,D]
    auto K_pad = pad_seq128(key);     // [B,Hk,N_pad,D]
    auto O_pad = at::zeros(Q_pad.sizes(), output.options().dtype(torch::kFloat)).contiguous();

    int M = query.size(2);
    int N = key.size(2);
    int M_pad = (query.size(2) + 127) / 128 * 128;
    int N_pad = (key.size(2) + 127) / 128 * 128;
    
    auto V_pad = value.contiguous();  // [B,H,D,N_padded]

    const int batch_size = Q_pad.size(0);
    const int head_dim = Q_pad.size(3);

    int stride_bz_q = Q_pad.stride(0);
    int stride_bz_k = K_pad.stride(0);
    int stride_bz_v = V_pad.stride(0);
    int stride_bz_o = O_pad.stride(0);

    int qo_len, kv_len, num_qo_heads, num_kv_heads;
    int stride_seq_q, stride_h_q, stride_seq_k, stride_h_k, stride_h_v, stride_d_v, stride_seq_o, stride_h_o;

    
        qo_len = Q_pad.size(2);
        kv_len = K_pad.size(2);
        num_qo_heads = Q_pad.size(1);
        num_kv_heads = K_pad.size(1);

        stride_seq_q = Q_pad.stride(2);
        stride_h_q = Q_pad.stride(1);
        stride_seq_k = K_pad.stride(2);
        stride_h_k = K_pad.stride(1);
        stride_h_v = V_pad.stride(1);
        stride_d_v = V_pad.stride(2);
        stride_seq_o = O_pad.stride(2);
        stride_h_o = O_pad.stride(1);
    

    if (num_qo_heads % num_kv_heads != 0) {
        std::ostringstream err_msg;
        err_msg << "num_qo_heads (" << num_qo_heads << ") must be divisible by num_kv_heads (" << num_kv_heads << ")";
        throw std::invalid_argument(err_msg.str());  
    }

    // here need add lse
    torch::Tensor lse = torch::empty({0});
    if (return_lse)
    {
        lse = torch::empty({batch_size, num_qo_heads, qo_len}, query.options().dtype(torch::kFloat32));
    }

    const int num_kv_groups = num_qo_heads / num_kv_heads;

    auto output_dtype = output.scalar_type();

    DISPATCH_HEAD_DIM(head_dim, HEAD_DIM, {
        DISPATCH_CAUSAL(is_causal, IS_CAUSAL, {
            DISPATCH_QK_QUANT_GRAN(qk_quant_gran, QK_QUANT_GRAN, {
                DISPATCH_RETURN_LSE(return_lse, RETURN_LSE, {
                    DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16(output_dtype, DTypeOut, {
                        constexpr int CTA_Q = 128;
                        constexpr int CTA_K = 64;
                        constexpr int WARP_Q = 32;
                        constexpr int WARP_K = 64;

                        assert(value.size(0) == batch_size);
                        // assert(value.size(3) >= div_ceil(kv_len, CTA_K) * CTA_K);

                        constexpr MaskMode mask_mode = IS_CAUSAL ? MaskMode::kCausal : MaskMode::kNone;

                        if constexpr (QK_QUANT_GRAN == static_cast<int>(QuantGranularity::kPerWarp))
                        {
                          CHECK_SHAPE(query_scale, batch_size, num_qo_heads, div_ceil(qo_len, CTA_Q) * (CTA_Q / WARP_Q));
                          CHECK_SHAPE(key_scale, batch_size, num_kv_heads, div_ceil(kv_len, CTA_K) * (CTA_K / WARP_K));
                        }
                        else if constexpr (QK_QUANT_GRAN == static_cast<int>(QuantGranularity::kPerThread))
                        {
                          CHECK_SHAPE(query_scale, batch_size, num_qo_heads, div_ceil(qo_len, CTA_Q) * (CTA_Q / WARP_Q) * 8);
                          CHECK_SHAPE(key_scale, batch_size, num_kv_heads, div_ceil(kv_len, CTA_K) * (CTA_K / WARP_K) * 4);    
                        }
                        else
                        {
                          static_assert(QK_QUANT_GRAN == static_cast<int>(QuantGranularity::kPerWarp) || QK_QUANT_GRAN == static_cast<int>(QuantGranularity::kPerThread), "Unsupported quantization granularity");
                        }

                        uint32_t ROCWMMA_M = Params::ROCWMMA_M;
                        uint32_t ROCWMMA_N = Params::ROCWMMA_N;
                        uint32_t ROCWMMA_K = Params::ROCWMMA_K;
                        uint32_t BLOCKS_X  = Params::BLOCKS_X;
                        uint32_t BLOCKS_Y  = Params::BLOCKS_Y;
                        uint32_t TBLOCK_X  = Params::TBLOCK_X;
                        uint32_t TBLOCK_Y  = Params::TBLOCK_Y;
                        uint32_t WARP_SIZE = Params::WARP_SIZE;

                        const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
                        const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
                        size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);

                        hipFuncSetAttribute(
                            (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
                                                                static_cast<QuantGranularity>(QK_QUANT_GRAN),
                                                                static_cast<QuantGranularity>(QK_QUANT_GRAN),
                                                                1, float, false, DTypeOut,
                                                                ComputeUnit::kCudaCore, mask_mode,
                                                                RETURN_LSE, false, false, false>,
                            hipFuncAttributeMaxDynamicSharedMemorySize,
                            (int)smem_max);
                        
                        dim3 grid(div_ceil(M_pad, Mx), num_qo_heads, batch_size);
                        dim3 block(TBLOCK_X, TBLOCK_Y);

                        hipLaunchKernelGGL((qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
                            static_cast<QuantGranularity>(QK_QUANT_GRAN),
                            static_cast<QuantGranularity>(QK_QUANT_GRAN),
                            HEAD_DIM / 64, float, false, DTypeOut,
                            ComputeUnit::kCudaCore, mask_mode,
                            RETURN_LSE, false, false, false>),
                            dim3(grid), dim3(block), smem_max, 0,
                            head_dim, head_dim, N_pad, head_dim,
                            Q_pad.data_ptr<int8_t>(), 
                            K_pad.data_ptr<int8_t>(),
                            reinterpret_cast<fp8_type*>(V_pad.data_ptr()),
                            reinterpret_cast<float*>(O_pad.data_ptr()),
                            (RETURN_LSE) ? reinterpret_cast<float*>(lse.data_ptr()) : nullptr,
                            reinterpret_cast<float*>(query_scale.data_ptr()),
                            reinterpret_cast<float*>(key_scale.data_ptr()),
                            reinterpret_cast<float*>(value_scale.data_ptr()),
                            nullptr,
                            M_pad, N_pad, N, 
                            num_kv_groups,
                            stride_bz_q, stride_seq_q, stride_h_q,
                            stride_bz_k, stride_seq_k, stride_h_k,
                            stride_bz_v, stride_h_v, stride_d_v,
                            stride_bz_o, stride_seq_o, stride_h_o,
                            sm_scale);

                        C10_HIP_CHECK(hipGetLastError());
                        auto O_valid = O_pad.narrow(/*dim=*/2, 0, M);
                        output.copy_(O_valid);
                    });
                });
            });
        });
    });
  return lse;
}